# -*- coding: utf-8 -*-
"""disaster_tweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hNVaj6S35hbP1n5WEZpl--i0HQittdWu

# Import Modules
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import string
import nltk
from nltk.stem import PorterStemmer

num_words = 20000
max_len = 15

"""# Load data"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Graduation_Project

!pwd

data = pd.read_csv('train.csv')
data

"""# Exploring the data"""

data.info()

# percentage of missing data in each feature

data.isnull().sum() * 100 / len(data)

# finding how many data in each class

data['target'].value_counts()

# top 15 locations in the data

data['location'].value_counts()[:15].index.tolist()

# top 15 keywords in the data

data['keyword'].value_counts()[:15].index.tolist()

"""# Removing unneeded columns"""

data = data.drop(['location', 'keyword', 'id'], axis=1)
data

"""# Data Cleaning"""

def cleanText(text):
    text=re.sub(r'#\S*','',text)  #  remove hashtag
    text=re.sub(r'@\S*','',text)  # remove user name 
    text=re.sub(r'https?:\/\/\S+','',text)  #remove hyperlinks
    text=re.sub('\d+','',text)  # remove digits
    text=re.sub(r'\W',' ',text) # remove emotions
    text=re.sub(r'^\s+','',text)  # remove space in front of text 
    text=re.sub(r'\s+$','',text)  # remove space in tail text
    return text

# split the text

def tokenization(text):
  tokens = re.split('\W+',text)
  return tokens

nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('english')

def remove_stopwords(text):
  stopwordsFree = [word for word in text if word not in stopwords]
  
  return stopwordsFree

def stemming(text):
  ps = PorterStemmer()
  output = [ps.stem(word) for word in text]

  return output

def process_data(text):
  clean_text = cleanText(text)
  clean_text = clean_text.lower()
  token = tokenization(clean_text)
  stopwords_free = remove_stopwords(token)
  output = stemming(stopwords_free)

  return output

# finding most common words & stopwords

sentences = data['text']

sentences = sentences.apply(lambda x: cleanText(x))
sentences = sentences.apply(lambda x: x.lower())
sentences = sentences.apply(lambda x: tokenization(x))

all_words = {}
all_stopwords = {}

for sentence in sentences:
  for word in sentence:

    if word not in stopwords:
      if word not in all_words:
        all_words[word] = 1
      else:
        all_words[word] += 1

    else:
      if word not in all_stopwords:
        all_stopwords[word] = 1
      else:
        all_stopwords[word] += 1

most_common_words = [key for key, value in all_words.items() if value == max(all_words.values())]
most_common_stopwords = [key for key, value in all_stopwords.items() if value == max(all_stopwords.values())]

print("Most common word is :", most_common_words[0])
print("Most common stopword is :", most_common_stopwords[0])

data['clean_text'] = data['text'].apply(lambda x: process_data(x))

data.head(50)

"""# Preparing train, val and test data"""

from sklearn.model_selection import train_test_split

X_train, X_val, Y_train, Y_val = train_test_split(data['clean_text'], data['target'], test_size=0.15, random_state=22)
print(X_train.shape)
print(X_val.shape)

test_data = pd.read_csv('test.csv')
X_test = test_data['text']

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer

toknizer=Tokenizer(num_words=num_words)
toknizer.fit_on_texts(X_train)
train_sequences = toknizer.texts_to_sequences(X_train)
print(train_sequences)

from tensorflow.keras.preprocessing.sequence import pad_sequences

padded_train_sequence=pad_sequences(train_sequences, truncating='post', maxlen=max_len)
print(padded_train_sequence)

val_sequences = toknizer.texts_to_sequences(X_val)
test_sequences = toknizer.texts_to_sequences(X_test)

padded_val_sequence = pad_sequences(val_sequences, truncating='post', maxlen=max_len)
padded_test_sequence = pad_sequences(test_sequences, truncating='post', maxlen=max_len)

"""# Build the model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

model = Sequential()
model.add(Embedding(num_words, 64, input_length=max_len))
model.add(LSTM(32, return_sequences=True))
model.add(Dropout(0.5))
model.add(LSTM(16))
model.add(Dropout(0.4))
model.add(Dense(1, activation='sigmoid'))

model.summary()

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('checkpoint.h5', monitor='val_accuracy', save_best_only=True)

model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])

history = model.fit(padded_train_sequence,Y_train, batch_size=128, epochs=4, validation_data=(padded_val_sequence, Y_val), callbacks=[model_checkpoint_callback])

plt.plot(history.history['accuracy'], 'b')
plt.plot(history.history['loss'], 'r')

"""# Test the model"""

predictions = model.predict(padded_test_sequence)
predictions = predictions.round()
predictions[:10]

"""# Save model"""

model.save('model.h5')